\section{Implementación y evaluación de las técnicas}
\label{sec:implementacion_y_evaluacion_de_las_tecnicas}

\subsection{Detalles de la implementación de las técnicas de DL aplicadas}

En este apartado cómo se ha implementado el proyecto, para ello se muestra una visión general de la estructura del código, se detallan y explican las distintas opciones de configuración para su ejecución y se explica un flujo completo de principio a fin: entrenamiento, evaluación, predicción, transformación y explotación en dispositivo móvil.

\subsubsection*{Estructura del proyecto}

Como se ha comentado anteriormente el proyecto se basa en dos implementaciones de YOLO (\textit{v3} y \textit{v3 tiny}) utilizando keras. Ambas implementaciones han sido unificadas en una que soporta ambos tipos de red. Dicha implementación presenta la estructura que se muestra a continuación (únicamente se muestran los elementos más relevantes y con un asterisco los scripts principales, el resto son auxiliares):

\begin{itemize}
	\item \texttt{utils}
	\begin{itemize}
		\item \texttt{bbox.py}
		\item \texttt{utils.py}
	\end{itemize}
	\item \texttt{annotations.py}
	\item \texttt{callbacks.py}
	\item \texttt{evaluate.py (*)}
	\item \texttt{predict.py (*)}
	\item \texttt{train.py (*)}
	\item \texttt{yolo\_generator.py}
	\item \texttt{yolo\_tiny\_generator.py}
	\item \texttt{yolo\_tiny\_weight\_reader.py}
	\item \texttt{yolo\_tiny.py}
	\item \texttt{yolo\_v3\_weight\_reader.py}
	\item \texttt{yolo.py}
\end{itemize}

A continuación se describe la funcionalidad general de cada uno de estos scripts:

\begin{itemize}
	\item \texttt{utils/bbox.py}: contiene la definición de la clase \texttt{BoundBox} que se utiliza para representar cada una de las predicciones obtenidas con el modelo. Tiene los atributos necesarios para identificar la región y la probabilidad de pertenencia del objeto a cada una de las clases.
	\item \texttt{utils/utils.py}: contiene funciones de cálculo auxiliares, como por ejemplo una función para calcular el \textit{AP} de un modelo, otra función para el procesamiento de la salida del modelo, etc.
	\item \texttt{annotations.py}: este script permite procesar los ficheros con las anotaciones de las imágenes. Es capaz de procesar annotaciones en formato \textit{VOC} y en formato \textit{txt}.
	\item \texttt{callbacks.py}: contiene la definición de \textit{callbacks} de keras customizados. Un ejemplo es un callback que se ha definido para que se ejecute al final de cada época para guardar el modelo actual si ha mejorado con respecto al anterior.
	\item \texttt{evaluate.py}: este script permite evaluar un modelo calculando su \textit{AP}. Recibe como parámetro un fichero de configuración.
	\item \texttt{predict.py}: este script permite obtener las predicciones de una imagen, las imágenes de un directorio o un video. Recibe como argumento un fichero de configuración.
	\item \texttt{train.py}: este script permite entrenar un modelo en base a un fichero de configuración, recibido como argumento.
	\item \texttt{yolo\_generator.py}: contiene la definición de la clase \texttt{BatchGenerator} que se utiliza para alimentar los modelos de tipo \textit{YOLO v3}. Tiene configuradas entre otras cosas un directorio que contiene imágenes y sus correspondientes anotaciones. Va proporcionando al modelo las entradas a medida que las va necesitando. Se encarga también del preprocesamiento de las imágenes, que en este caso consiste en redimensionar la imagen al tamaño de la red neuronal y hacer ciertas transformaciones aleatorias sobre la imagen, como por ejemplo rotarla.
	\item \texttt{yolo\_v3\_weight\_reader.py}: contiene la definición de la clase \texttt{WeightReader}  que es capaz de transformar los pesos ya entrenados de la red \textit{YOLO v3}.
	\item \texttt{yolo.py}:  contiene la definición del modelo \textit{YOLO v3} con cada una de sus capas convolucionales y sus interconexiones.
	\item \texttt{yolo\_tiny\_generator.py}: contiene la definición de la clase \texttt{BatchGenerator}, pero adaptada para los modelos \textit{YOLO v3 tiny}.
	\item \texttt{yolo\_tiny\_weight\_reader.py}: contiene la de la clase \texttt{WeightReader} que es capaz de transformar los pesos ya entrenados de la red \textit{YOLO v3 tiny}, que están en formato \textit{darknet}, al formato de keras.
	\item \texttt{yolo\_tiny.py}: contiene la definición del modelo \textit{YOLO v3} con cada una de sus capas convolucionales y sus interconexiones.
\end{itemize}

\subsubsection*{Configuración}

Tanto el entrenamiento, como la evaluación, como la predicción de imágenes se basan en un fichero de configuración que tiene el siguiente formato:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Configuración de ejemplo}, captionpos=b]
{
  "model": {
    "type": "tiny",
    "min_input_size": 256,
    "max_input_size": 256,
    "anchors": [ 17,6, 21,9, 24,6, 27,8, 33,11, 36,17 ],
    "labels": [ "pothole" ],
    "data_load_method": "txt"
  },
  "train": {
    "train_image_folder": "<PATH_TO_DIR>",
    "train_annot": "<PATH_TO_DIR_OR_FILE>",
    "cache_name": "<PATH_TO_FILE>",
    "train_times": 4,
    "batch_size": 4,
    "learning_rate": 0.0001,
    "nb_epochs": 20,
    "warmup_epochs": 3,
    "ignore_thresh": 0.5,
    "early_stopping_patience": 12,
    "reduce_lr_on_plateau_patience": 4,
    "gpus": "0",
    "grid_scales": [ 1, 1 ],
    "obj_scale": 5,
    "noobj_scale": 1,
    "xywh_scale": 1,
    "class_scale": 1,
    "tensorboard_dir": "logs/",
    "saved_weights_name": "<PATH_WHERE_TRAINED_MODEL_IS_SAVED>",
    "pretrained_weights": "<PATH_TO_PRETRAINED_MODEL_FILE>",
    "debug": true
  },
  "valid": {
    "valid_image_folder": "<PATH_TO_DIR>",
    "valid_annot": "<PATH_TO_DIR_OR_FILE>",
    "cache_name": "<PATH_TO_FILE>",
    "duplicate_thresh": 0.20,
    "valid_times": 1
  }
}
\end{lstlisting}

La configuración está dividida en tres secciones. La sección \texttt{model} donde se encuentran configuraciones generales del modelo. La sección \texttt{train} donde se encuentran las configuraciones para la fase de entrenamiento. Y por último la sección \texttt{valid} donde se encuentran las configuraciones para la evaluación del modelo entrenado.

A continuación se describen las principales propiedades de configuración:

\begin{itemize}
	\item \texttt{model.type}: tipo de modelo que se quiere entrenar. Se soportan dos tipos: \texttt{v3} y \texttt{tiny}, que se corresponden con \textit{YOLO v3} y \textit{YOLO v3 tiny} respectivamente.
	\item \texttt{model.min\_input\_size} y \texttt{model.max\_input\_size}: tamaño mínimo y máximo de la red. La red neuronal YOLO en su versión 3 es una red neuronal de tamaño flexible. Cada 10 imágenes procesadas durante el entrenamiento, se redimensionarán las imágenes a un tamaño aleatorio entre el mínimo y el máximo configurados, adaptándose en consecuencia el tamaño de la red.
	\item \texttt{model.anchors}: lista de los las relaciones de aspecto ancho-alto más frecuentes de los objetos a detectar. Estos anchors son usados por YOLO para proponer las regiones. Existe otro script, no mencionado anteriormente (\texttt{gen\_anchors.py}), que permite obtener esta lista aplicando \textit{k-means} sobre las anotaciones de las imágenes.
	\item \texttt{model.labels}: lista con los nombres de las clases de los objetos etiquetados en las imágenes.
	\item \texttt{model.data\_load\_method}: formato en el que están representadas las etiquetas de las imágenes. Acepta los valores \texttt{txt} y \texttt{voc} (XML).
	\item \texttt{train.train\_image\_folder}: ruta al directorio donde se encuentran las imágenes de entrenamiento.
	\item \texttt{train.train\_annot}: ruta a un directorio (en el caso de anotaciones \textit{voc}) o a un fichero (en el caso de anotaciones \textit{txt}) con las anotaciones de las imágenes de entrenamiento.
	\item \texttt{train.cache\_name}: ruta a un fichero que sirve de caché para las imágenes de entrenamiento. Esta caché, además de contener las ubicaciones de las imágenes y las anotaciones de las mismas, también contiene las dimensiones de las imágenes. Estas dimensiones son necesarias cuando hay que hacer una redimensión de la imagen, para poder adaptar las regiones de las anotaciones en consonancia. Esta caché evita tener que volver a leer cada una de las imágenes para volver a obtener sus dimensiones.
	\item \texttt{valid.valid\_image\_folder}, \texttt{valid.valid\_annot} y \texttt{valid.cache\_name}: son propiedades análogas a \texttt{train.train\_image\_folder}, \texttt{train.train\_annot} y \texttt{train.cache\_name} respectivamente, pero para las imágenes de test.
	\item \texttt{train.saved\_weights\_name}: ruta donde se va a guardar el modelo entrenado. Después de cada época, si el modelo ha mejorado con respecto a la época anterior, será guardado en la ruta configurada, reemplazando al anterior.
	\item \texttt{train.pretrained\_weights}: ruta con un modelo preentrenado. Esta propiedad permite hacer una transferencia de conocimiento, bien partiendo de un modelo entrenado para detectar otros objetos distintos, o bien para continuar con un entrenamiento anterior.
	\item \texttt{train.batch\_size}: tamaño del bloque de imágenes después del cual se aplica \textit{backpropagation}.
	\item \texttt{train.nb\_epochs}: número de veces que el modelo es entrenado con todo el conjunto de entrenamiento.
	\item \texttt{train.learning\_rate}: número que permite ajustar cómo de rápido aprende el modelo.
	\item \texttt{train.early\_stopping\_patience}: número de épocas que tienen que pasar sin que el modelo mejore para que se detenga el aprendizaje de manera automática, aunque no se haya llegado al número de épocas configuradas.
	\item \texttt{train.reduce\_lr\_on\_plateau\_patience}: número de épocas que tienen que pasar sin que el modelo mejore para que se reduzca el \texttt{train.learning\_rate} de manera automática.
	\item \texttt{train.ignore\_thresh}: umbral por debajo del cual una predicción es ignorada por lo representar suficientemente a ninguno de los objetos a detectar. Se utiliza la \textit{IoU} como unidad de medida. Si el valor de \textit{IoU} entre la región predicha y las regiones a detectar es inferior al límite configurado, la predicción es rechazada.
	\item \texttt{valid.duplicate\_thresh}: umbral por encima del cual dos predicciones se consideran la misma. Se utiliza la \textit{IoU} como unidad de medida. Si el valor de \textit{IoU} entre varias de las regiones predichas para una misma imagen es superior al umbral configurado, se considerarán la misma predicción y prevalecerá la que tenga un valor más alto de \textit{IoU} con respecto a la región a detectar. Esto es lo que se conoce como \textit{Non-Maximun Suppression} \cite{s6_nonmaximunsuppression}
	\item \texttt{train.train\_times}: número de veces que se recorre el conjunto de entrenamiento por cada época. Esto es útil cuando el conjunto de entrenamiento contiene pocas imágenes.
\end{itemize}

\subsubsection*{Formatos para las etiquetas}

Se soportan dos formatos para las etiquetas de las imágenes: \texttt{txt} y \texttt{voc}. El formato \texttt{txt} consiste en un fichero de texto con una línea por cada imagen con el siguiente aspecto:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Ejemplo de fichero con las etiquetas en formato txt. La primera de las imágenes tiene 2 etiquetas, la primera está ubicada en la posición (100, 100) y tiene unas dimensiones de 100x40 píxeles. La segunda etiqueta está ubicada en la posición (200, 200) y tiene unas dimensiones de 75x30 píxeles. La segunda imagen tiene una única etiqueta ubicada en (300, 300) con dimensiones 250x100. Todas las etiquetas son de la clase 1}, captionpos=b]
/tmp/imagen1.jpg 100,100,100,40,1 200,200,75,30,1
/tmp/imagen2.jpg 300,300,250,100,1
\end{lstlisting}

En el formato \texttt{voc} existe un fichero \textit{xml} para cada una de las imágenes con el siguiente aspecto:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Ejemplo de fichero con las etiquetas en formato voc}, captionpos=b]
<annotation>
  <folder>/tmp</folder>
  <filename>imagen1.jpg</filename>
  <size>
    <width>3680</width>
    <height>2760</height>
    <depth>3</depth>
  </size>
  <object>
    <name>pothole</name>
    <difficult>0</difficult>
    <bndbox>
      <xmin>100</xmin>
      <ymin>100</ymin>
      <xmax>200</xmax>
      <ymax>140</ymax>
    </bndbox>
  </object>
</annotation>
\end{lstlisting}

\subsubsection*{Generación de \textit{anchors}}

Como se ha comentado anteriormente, en la descripción de la configuración, existe el script \texttt{gen\_anchors.py} que permite obtener un número determinado de anchors para un conjunto de imágenes. Para poder obtener este listado hace falta:

\begin{itemize}
	\item Disponer de un conjunto de imágenes
	\item Disponer de las etiquetas de las imágenes en formato \texttt{txt} (el formato \texttt{voc} no está sorportado en este caso)
	\item Haber creado un fichero de configuración. En esta configuración el atributo \texttt{model.anchors} es irrelevante, porque es lo que se va a generar
\end{itemize}

Con todo lo anterior se dispone de lo necesario para ejecutar el script \texttt{gen\_anchors.py}:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Cómo generar los \textit{anchors}}, captionpos=b]
> python gen_anchors.py --config config.json --anchors 9
\end{lstlisting}

Como resultado se obtendrá una lista con tantos anchors como se hayan indicado. Cada anchor está compuesto por una pareja de números, que representan el ancho y el alto del anchor.

\subsubsection*{Entrenamiento}

Para poder entrenar un modelo hace falta lo siguiente:

\begin{itemize}
	\item Disponer de un conjunto de imágenes de entrenamiento
	\item Disponer de las etiquetas de las imágenes en uno de los formatos soportados
	\item Haber generado los \textit{anchors} para las imágenes de entrenamiento
	\item Haber creado un fichero de configuración
\end{itemize}

Una vez que se dispone de todo lo anterior, lo único que queda por hacer es ejecutar el script \texttt{train.py}:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Cómo lanzar el entrenamiento}, captionpos=b]
> python train.py --config config.json
\end{lstlisting}

Una vez finaliza el entrenamiento, y si se ha configurado el bloque \texttt{valid} en el fichero de configuración, se realizará una evaluación del modelo obtenido.

\subsubsection*{Evaluación}

Para poder evaluar un modelo entrenado hará falta:

\begin{itemize}
	\item Disponer de un conjunto de imágenes de test
	\item Disponer de las etiquetas de las imágenes en uno de los formatos soportados
	\item Disponer de un modelo ya entrenado
	\item Haber creado un fichero de configuración
\end{itemize}

Una vez se dispone de todo lo anterior, lo único que habrá que hacer es ejecutar el script \texttt{evaluate.py}:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Cómo evaluar un modelo entrenado}, captionpos=b]
> python evaluate.py --config config.json
\end{lstlisting}

La evaluación hace el cálculo de la \textit{AP} tanto de una forma global como para cada una de las clases que se hayan configurado.

\subsubsection*{Predicción}

Para realizar una predicción con el modelo ya entrenado es necesario:

\begin{itemize}
	\item Disponer de una imagen o directorio con imágenes o vídeo
	\item Disponer de un modelo ya entrenado
	\item Haber creado un fichero de configuración
\end{itemize}

Con todo lo anterior, se ejecuta el script \texttt{predict.py}:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Cómo hacer una predicción con un modelo entrenado}, captionpos=b]
# para una imagen
> python predict.py -c config.json -i /tmp/imagen.jpg -o /tmp/pred/

# para un video
> python predict.py -c config.json -i /tmp/video.mp4 -o /tmp/pred/

# para un conjunto de imagenes
> python predict.py -c config.json -i /tmp/imagenes -o /tmp/pred/
\end{lstlisting}

En el directorio de salida habrá el mismo contenido que en la entrada modificado con las etiquetas que se han encontrado.

\subsubsection*{Transformación del modelo}

Para explotar un modelo entrenado en un dispositivo móvil es necesario realizar una transformación del mismo. Para realizar esta transformación, \textit{TFLite} que proporciona una serie de utilidades. En el siguiente bloque de código se muestra un ejemplo:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, language=Python, caption={Cómo transformar un modelo entrenado con \textit{Keras} a formato \textit{TFLite}}, captionpos=b]
import tensorflow as tf

tflite_converter = tf.lite.TFLiteConverter
  .from_keras_model_file(<KERAS_H5_MODEL_PATH>)

tflite_model = tflite_converter.convert()

with open(<KERAS_TFLITE_DEST_PATH>, 'wb') as tflite_model_file:
  tflite_model_file.write(tflite_model)
\end{lstlisting}

\subsubsection*{Explotación del modelo}

\textit{TFLite} además de proporcionar herramientas para transformar modelos también proporciona librerías para explotarlos de diversas formas. En concreto, proporciona una librería java, que entre otras cosas, permite explotar un modelo en un dispositivo móvil \textit{Android}.

En este primer bloque de código se muestra un ejemplo de cómo se puede cargar un modelo que forma parte de los \textit{assets} de la aplicación móvil:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, language=Java, caption={Cómo cargar un modelo}, captionpos=b]
AssetFileDescriptor fileDescriptor = assets
                                 .openFd(<MODEL_TFLITE_FILENAME>);
FileInputStream inputStream = new FileInputStream(fileDescriptor
                                 .getFileDescriptor());
FileChannel fileChannel = inputStream.getChannel();
long startOffset = fileDescriptor.getStartOffset();
long declaredLength = fileDescriptor.getDeclaredLength();
MappedByteBuffer model = fileChannel.map(
                                 FileChannel.MapMode.READ_ONLY,
                                 startOffset, declaredLength);

Interpreter.Options interpreterOptions = new Interpreter.Options()
                                           .setNumThreads(1);
Interpreter tfLite = new Interpreter(model, interpreterOptions);
\end{lstlisting}

En la variable \texttt{tflite} se tiene disponible el modelo ya cargado, listo para ser explotado. Para ello, hay que proporcionarle los vectores que se espera como entrada e indicarle los vectores donde devolver el resultado. En el siguiente ejemplo se crea un único vector de entrada de dimensiones $1 x 256 x 256 x 3$, que se correspondería con una imagen a color de $256x256$. Y se crean 2 vectores de salida de tamaños $8 x 8 x 18$ y $16 x 16 x 18$, que se corresponden con los vectores de salida de la versión YOLO \textit{v3 tiny} con un tamaño de $256$:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, language=Java, caption={Creación de vectores de entrada y de salida del modelo}, captionpos=b]
float[][][][] floatValues = new float[1][256][256][3];

for (int i = 0; i < 256; i++) {
  for (int j = 0; j < 256; j++) {
    for (int k = 0; k < 3; k++) {
      floatValues[0][i][j][k] = 1.0f;
    }
  }
}

Object[] inputArray = {floatValues};

output1 = new float[1][8][8][18];
output2 = new float[1][16][16][18];

Map<Integer, Object> outputMap = new HashMap<>();
outputMap.put(0, output1);
outputMap.put(1, output2);
\end{lstlisting}

El modelo se ejecuta de la siguiente manera:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, language=Java, caption={Cómo ejecutar el modelo}, captionpos=b]
tfLite.runForMultipleInputsOutputs(inputArray, outputMap);
\end{lstlisting}

Una vez termine de ejecutarse el método \texttt{runForMultipleInputsOutputs}, en la variable \texttt{outputMap} estará disponible la salida del modelo lista para ser interpretada.

\subsection{Evaluación de las técnicas}

Se han entrenado dos versiones de YOLO: la versión \textit{v3} y la versión \textit{v3 tiny}. La versión \textit{v3 tiny} es la versión indicada para ser ejecutada en un dispositivo móvil. La versión \textit{v3} se ha entrenado para compararla con la tiny.

Para cada una de estas versiones se han entrenado varios modelos con distintos tamaños de red, por dos motivos principalmente: por una cuestión de rendimiento a la hora de ejecutar el modelo en un dispositivo móvil y por analizar cómo varía la precisión del modelo cambiando el tamaño de la red.

Además se han utilizado distintos conjuntos de entrenamiento para entrenar todas las variantes del modelo. El primero de los conjuntos de entrenamiento se corresponde con el conjunto íntegro original (denominado \textit{completo}). Los resultados obtenidos con este conjunto de entrenamiento obtuvieron unos valores bajos para la métrica \textit{AP}, y tras analizar los motivos, se observó que había una gran cantidad de baches demasiado pequeños que podían ser los causantes malos resultados. Por este motivo, se han utilizado dos conjuntos de entrenamiento adicionales aplicando filtros sobre los baches. En el primero de estos conjuntos de entrenamiento adicionales se han filtrado los baches con tamaño superior a 75x30 píxeles (denominado \textit{filtro 75x30}) y en el segundo se han filtrado los baches con tamaño superior a 100x40 píxeles (denominado \textit{filtro 100x40}). Para cada uno de estos conjuntos de entrenamiento adicionales se ha creado también su correspondiente conjunto de evaluación aplicando el mismo filtro.

Con todos los modelos resultantes obtenidos se ha realizado una doble evaluación. Por un lado se han evaluado con los conjuntos de evaluación correspondientes para cada uno de los conjuntos de entrenamiento (resultados en la tabla \ref{tab:evaluationoriginal}). Por otro lado se han evaluado con un conjunto de imágenes generado (resultados en la tabla \ref{tab:evaluationcustom}). Este conjunto de evaluación (denominado \textit{propio}) se compone de unas 30 imágenes de 4032x3024 píxeles, con unos 60 baches en total, obtenido desde la acera (a diferencia del original que fue obtenido desde el coche) y compuesto por fotos realizadas en España (a diferencia del original que fueron realizadas en Sudáfrica).

\begin{table}[H]
	\centering
	\begin{tabular}{lrlrr}
		\toprule
		Versión YOLO &  Tamaño &    Juego datos &  Épocas &  Mejor AP \\
		\midrule
		V3      &     256 &       completo &      43 &    0.0747 \\
		V3      &     256 &  filtro 100x40 &      93 &    0.3077 \\
		V3      &     256 &   filtro 75x30 &      88 &    0.2513 \\
		V3      &     416 &       completo &      18 &    0.1467 \\
		V3      &     416 &  filtro 100x40 &      93 &    0.4161 \\
		V3      &     416 &   filtro 75x30 &      93 &    0.3611 \\
		V3      &     640 &       completo &      13 &    0.0186 \\
		V3      &     640 &  filtro 100x40 &      63 &    0.5475 \\
		V3      &     640 &   filtro 75x30 &      53 &    0.4106 \\
		V3 Tiny &     256 &       completo &     144 &    0.0046 \\
		V3 Tiny &     256 &  filtro 100x40 &     136 &    0.0510 \\
		V3 Tiny &     256 &   filtro 75x30 &     153 &    0.0392 \\
		V3 Tiny &     416 &       completo &     153 &    0.0145 \\
		V3 Tiny &     416 &  filtro 100x40 &     153 &    0.1307 \\
		V3 Tiny &     416 &   filtro 75x30 &     146 &    0.0869 \\
		\bottomrule
	\end{tabular}
	\caption{Resultados obtenidos con los conjuntos de evaluación originales}
	\label{tab:evaluationoriginal}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{lrlrr}
		\toprule
		Versión YOLO &  Tamaño &    Juego datos &  Épocas &  Mejor AP \\
		\midrule
		V3      &     256 &       propio completo &      43 &    0.0289 \\
		V3      &     256 &  propio filtro 100x40 &      93 &    0.1018 \\
		V3      &     256 &   propio filtro 75x30 &      88 &    0.0179 \\
		V3      &     416 &       propio completo &      18 &    0.0354 \\
		V3      &     416 &  propio filtro 100x40 &      93 &    0.0089 \\
		V3      &     416 &   propio filtro 75x30 &      93 &    0.0294 \\
		V3      &     640 &       propio completo &      13 &    0.0017 \\
		V3      &     640 &  propio filtro 100x40 &      63 &    0.0342 \\
		V3      &     640 &   propio filtro 75x30 &      53 &    0.0961 \\
		V3 Tiny &     256 &       propio completo &     144 &    0.0086 \\
		V3 Tiny &     256 &  propio filtro 100x40 &     136 &    0.0232 \\
		V3 Tiny &     256 &   propio filtro 75x30 &     153 &    0.0371 \\
		V3 Tiny &     416 &       propio completo &     153 &    0.0000 \\
		V3 Tiny &     416 &  propio filtro 100x40 &     153 &    0.0000 \\
		V3 Tiny &     416 &   propio filtro 75x30 &     146 &    0.0006 \\
		\bottomrule
	\end{tabular}
	\caption{Resultados obtenidos con el conjunto de evaluación propio}
	\label{tab:evaluationcustom}
\end{table}