\section{Implementación y evaluación de las técnicas}
\label{sec:implementacion_y_evaluacion_de_las_tecnicas}

\subsection{Implementación de las técnicas de Deep Learning aplicadas en el proyecto}

En este apartado se explica cómo se ha implementado el proyecto. Para ello se muestra una visión general de la estructura del código, se detallan y explican las distintas opciones de configuración para su ejecución y se explica un flujo completo de principio a fin: entrenamiento, evaluación, predicción, transformación y explotación en dispositivo móvil.

\subsubsection*{Estructura del proyecto}

Como se ha comentado anteriormente el proyecto se basa en dos implementaciones de YOLO en Keras (V3 y V3 Tiny). Ambas implementaciones han sido unificadas en una \footnote{https://github.com/dicastro/tfm/tree/tfm-yolo3-tiny} que soporta ambos tipos de red. Dicha implementación presenta la estructura que se muestra a continuación (únicamente se muestran los elementos más relevantes y con un asterisco los scripts principales, el resto son auxiliares):

\begin{itemize}
	\item \texttt{utils}
	\begin{itemize}
		\item \texttt{bbox.py}
		\item \texttt{utils.py}
	\end{itemize}
	\item \texttt{annotations.py}
	\item \texttt{callbacks.py}
	\item \texttt{evaluate.py (*)}
	\item \texttt{predict.py (*)}
	\item \texttt{train.py (*)}
	\item \texttt{yolo\_generator.py}
	\item \texttt{yolo\_tiny\_generator.py}
	\item \texttt{yolo\_tiny\_weight\_reader.py}
	\item \texttt{yolo\_tiny.py}
	\item \texttt{yolo\_v3\_weight\_reader.py}
	\item \texttt{yolo.py}
\end{itemize}

A continuación se describe la funcionalidad general de cada uno de estos scripts:

\begin{itemize}
	\item \texttt{utils/bbox.py}: contiene la definición de la clase \texttt{BoundBox} que se utiliza para representar cada una de las predicciones obtenidas. Tiene los atributos necesarios para identificar la región y la probabilidad de pertenencia del objeto a cada una de las clases.
	\item \texttt{utils/utils.py}: contiene funciones de cálculo auxiliares, como por ejemplo una función para calcular la \textit{AP} de un modelo, otra función para el procesamiento de la salida del modelo, etc.
	\item \texttt{annotations.py}: este script permite procesar los ficheros con las etiquetas de las imágenes. Es capaz de procesar annotaciones en formato \texttt{voc} y en formato \texttt{txt}.
	\item \texttt{callbacks.py}: contiene la definición de unos \textit{callbacks} de keras customizados. Como por ejemplo, un callback que se ha definido para que se ejecute al final de cada época y persista en disco el modelo actual siempre que mejore con respecto al de la época anterior.
	\item \texttt{evaluate.py}: este script permite evaluar un modelo calculando su \textit{AP}.
	\item \texttt{predict.py}: este script permite obtener las predicciones a partir de una imagen, de las imágenes de un directorio o de un video.
	\item \texttt{train.py}: este script permite entrenar un modelo.
	\item \texttt{yolo\_generator.py}: contiene la definición de la clase \texttt{BatchGenerator} que se utiliza para alimentar los modelos YOLO V3. Tiene configurado, entre otras cosas, un directorio que contiene imágenes y sus correspondientes anotaciones. Se encarga de proporcionar al modelo las imágenes a medida que las va necesitando. Es muy útil cuando se tienen grandes cantidades de datos, ya que evita tener que cargar todos los datos en memoria. Se encarga también del preprocesamiento de las imágenes, que en este caso consiste en redimensionar la imagen al tamaño de la red neuronal y normalizarla dividiendo entre $255$. Además, durante la fase de entrenamiento, realiza otro tipo de transformaciones aleatorias, como por ejemplo rotaciones, cambios de saturación o de color.
	\item \texttt{yolo\_v3\_weight\_reader.py}: contiene la definición de la clase \texttt{WeightReader}  que es capaz de transformar los pesos ya entrenados de la red YOLO V3 en formato \textit{darknet} a formato \textit{keras}.
	\item \texttt{yolo.py}:  contiene la definición en keras del modelo YOLO V3.
	\item \texttt{yolo\_tiny\_generator.py}: contiene la definición de la clase \texttt{BatchGenerator}, pero adaptada para los modelos YOLO V3 Tiny.
	\item \texttt{yolo\_tiny\_weight\_reader.py}: contiene la definición de la clase \texttt{WeightReader}  que es capaz de transformar los pesos ya entrenados de la red YOLO V3 Tiny en formato \textit{darknet} a formato \textit{keras}.
	\item \texttt{yolo\_tiny.py}: contiene la definición en keras del modelo YOLO V3 Tiny.
\end{itemize}

\subsubsection*{Configuración}

Tanto el entrenamiento, como la evaluación, como la predicción de imágenes se basan en un fichero de configuración que hay que pasar como argumento y que tiene el siguiente formato:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Configuración de ejemplo}, captionpos=b]
{
  "model": {
    "type": "tiny",
    "min_input_size": 256,
    "max_input_size": 256,
    "anchors": [ 17,6, 21,9, 24,6, 27,8, 33,11, 36,17 ],
    "labels": [ "pothole" ],
    "data_load_method": "txt"
  },
  "train": {
    "train_image_folder": "/dataset/train",
    "train_annot": "/dataset/train_annotations.txt",
    "cache_name": "/dataset/train_cache.pickle",
    "train_times": 4,
    "batch_size": 4,
    "learning_rate": 0.0001,
    "nb_epochs": 20,
    "warmup_epochs": 3,
    "ignore_thresh": 0.5,
    "early_stopping_patience": 12,
    "reduce_lr_on_plateau_patience": 4,
    "gpus": "0",
    "grid_scales": [ 1, 1 ],
    "obj_scale": 5,
    "noobj_scale": 1,
    "xywh_scale": 1,
    "class_scale": 1,
    "tensorboard_dir": "logs/",
    "saved_weights_name": "/model/v1.h5",
    "pretrained_weights": "/model/yolo_v3_tiny_weights.h5",
    "debug": true
  },
  "valid": {
    "valid_image_folder": "/dataset/test",
    "valid_annot": "/dataset/test_annotations.txt",
    "cache_name": "/dataset/test_cache.pickle",
    "duplicate_thresh": 0.20,
    "valid_times": 1
  }
}
\end{lstlisting}

La configuración está dividida en tres secciones. La sección \texttt{model} donde se encuentran configuraciones generales del modelo. La sección \texttt{train} donde se encuentran las configuraciones para la fase de entrenamiento. Y por último la sección \texttt{valid} donde se encuentran las configuraciones para la evaluación del modelo entrenado.

A continuación se describen las principales propiedades de configuración:

\begin{itemize}
	\item \texttt{model.type}: tipo de modelo que se quiere utilizar. Admite los valores: \texttt{v3} y \texttt{tiny}, que se corresponden con YOLO V3 y YOLO V3 Tiny respectivamente.
	\item \texttt{model.min\_input\_size} y \texttt{model.max\_input\_size}: tamaño mínimo y máximo de la red. La red neuronal YOLO en su versión 3 es una red neuronal de tamaño variable. Durante la fase de entrenamiento se cambia el tamaño de la red cada 10 imágenes procesadas. Se establece de forma aleatoria un nuevo tamaño entre los rangos configurados.
	\item \texttt{model.anchors}: lista con las relaciones de aspecto ancho-alto más frecuentes de los objetos a detectar. Estos anchors son usados por YOLO para proponer las regiones. Con el script \texttt{gen\_anchors.py} se puede obtener fácilmente esta lista. Lo que hace el script es aplicar \textit{k-means} sobre las etiquetas de las imágenes.
	\item \texttt{model.labels}: lista con los nombres de las clases de los objetos etiquetados en las imágenes.
	\item \texttt{model.data\_load\_method}: formato en el que están representadas las etiquetas de las imágenes. Acepta los valores \texttt{txt} y \texttt{voc} (XML).
	\item \texttt{train.train\_image\_folder}: ruta al directorio donde se encuentran las imágenes de entrenamiento.
	\item \texttt{train.train\_annot}: ruta a un directorio (en el caso de anotaciones \textit{voc}) o a un fichero (en el caso de anotaciones \textit{txt}) con las anotaciones de las imágenes de entrenamiento.
	\item \texttt{train.cache\_name}: ruta a un fichero que sirve de caché para las imágenes de entrenamiento. Esta caché, además de contener las ubicaciones de las imágenes y las anotaciones de las mismas, también contiene las dimensiones de las imágenes. Estas dimensiones son necesarias cuando hay que hacer una redimensión de la imagen, para poder adaptar las regiones de las anotaciones en consonancia. Esta caché evita tener que volver a leer cada una de las imágenes para volver a obtener sus dimensiones.
	\item \texttt{valid.valid\_image\_folder}, \texttt{valid.valid\_annot} y \texttt{valid.cache\_name}: son propiedades análogas a \texttt{train.train\_image\_folder}, \texttt{train.train\_annot} y \texttt{train.cache\_name} respectivamente, pero para las imágenes de test.
	\item \texttt{train.saved\_weights\_name}: ruta donde se va a guardar el modelo entrenado. Después de cada época, si el modelo ha mejorado con respecto a la época anterior, será guardado en la ruta configurada, reemplazando al anterior.
	\item \texttt{train.pretrained\_weights}: ruta con un modelo preentrenado. Esta propiedad permite hacer una transferencia de conocimiento, bien partiendo de un modelo entrenado para detectar otros objetos distintos, o bien para continuar con un entrenamiento anterior.
	\item \texttt{train.batch\_size}: tamaño del bloque de imágenes después del cual se aplica \textit{backpropagation}.
	\item \texttt{train.nb\_epochs}: número de veces que el modelo es entrenado con todo el conjunto de entrenamiento.
	\item \texttt{train.learning\_rate}: número que permite ajustar cómo de rápido aprende el modelo.
	\item \texttt{train.early\_stopping\_patience}: número de épocas que tienen que pasar sin que el modelo mejore para que se detenga el aprendizaje de manera automática, aunque no se haya llegado al número de épocas configuradas.
	\item \texttt{train.reduce\_lr\_on\_plateau\_patience}: número de épocas que tienen que pasar sin que el modelo mejore para que se reduzca el \texttt{train.learning\_rate} de manera automática.
	\item \texttt{train.ignore\_thresh}: umbral por debajo del cual una predicción es ignorada por no representar suficientemente a ninguno de los objetos a detectar. Se utiliza la \textit{IoU} como unidad de medida. Si el valor de \textit{IoU} entre la región predicha y las regiones a detectar es inferior al límite configurado, la predicción es rechazada.
	\item \texttt{valid.duplicate\_thresh}: umbral por encima del cual dos predicciones se consideran la misma. Se utiliza la \textit{IoU} como unidad de medida. Si el valor de \textit{IoU} entre varias de las regiones predichas para una misma imagen es superior al umbral configurado, se considerarán la misma predicción y prevalecerá la que tenga un valor más alto de \textit{IoU} con respecto a la región a detectar. Esto es lo que se conoce como \textit{Non-Maximun Suppression} \cite{s6_nonmaximunsuppression}
	\item \texttt{train.train\_times}: número de veces que se recorre el conjunto de entrenamiento por cada época. Esto es útil cuando el conjunto de entrenamiento contiene pocas imágenes.
\end{itemize}

\subsubsection*{Formatos para las etiquetas}

Se soportan dos formatos para las etiquetas de las imágenes: \texttt{txt} y \texttt{voc}. El formato \texttt{txt} consiste en un fichero de texto con una línea por cada imagen con el siguiente aspecto:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Ejemplo de fichero con las etiquetas en formato txt. La primera de las imágenes tiene 2 etiquetas, la primera está ubicada en la posición (100, 100) y tiene unas dimensiones de 100x40 píxeles. La segunda etiqueta está ubicada en la posición (200, 200) y tiene unas dimensiones de 75x30 píxeles. La segunda imagen tiene una única etiqueta ubicada en (300, 300) con dimensiones 250x100. Todas las etiquetas son de la clase 1}, captionpos=b]
/tmp/imagen1.jpg 100,100,100,40,1 200,200,75,30,1
/tmp/imagen2.jpg 300,300,250,100,1
\end{lstlisting}

En el formato \texttt{voc} existe un fichero \textit{xml} para cada una de las imágenes con el siguiente aspecto:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Ejemplo de fichero con las etiquetas en formato voc para una imagen}, captionpos=b]
<annotation>
  <folder>/tmp</folder>
  <filename>imagen1.jpg</filename>
  <size>
    <width>3680</width>
    <height>2760</height>
    <depth>3</depth>
  </size>
  <object>
    <name>pothole</name>
    <difficult>0</difficult>
    <bndbox>
      <xmin>100</xmin>
      <ymin>100</ymin>
      <xmax>200</xmax>
      <ymax>140</ymax>
    </bndbox>
  </object>
</annotation>
\end{lstlisting}

\subsubsection*{Generación de \textit{anchors}}

Como se ha comentado anteriormente, en la descripción de la configuración, existe el script \texttt{gen\_anchors.py} que permite obtener un número determinado de anchors para un conjunto de imágenes. Para poder obtener este listado hace falta:

\begin{itemize}
	\item Disponer de un conjunto de imágenes
	\item Disponer de las etiquetas de las imágenes en formato \texttt{txt} (el formato \texttt{voc} no está sorportado en este caso)
	\item Haber creado un fichero de configuración. En esta configuración el atributo \texttt{model.anchors} es irrelevante, porque es lo que se va a generar
\end{itemize}

Con todo lo anterior se dispone de lo necesario para ejecutar el script \texttt{gen\_anchors.py}:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Cómo generar los \textit{anchors}}, captionpos=b]
> python gen_anchors.py --config config.json --anchors 9
\end{lstlisting}

Como resultado se obtendrá una lista con tantos anchors como se hayan indicado. Cada anchor está compuesto por una pareja de números, que representan el ancho y el alto del anchor. El número de anchors a generar dependerá de la versión de YOLO. Para YOLO V3 hacen falta 9 anchors, 3 anchors para cada una de las escalas de detección de objetos. Para YOLO V3 Tiny harán falta 6 anchors, ya que únicamente tiene 2 escalas de detección.

\subsubsection*{Entrenamiento}

Para entrenar un modelo es necesario:

\begin{itemize}
	\item Disponer de un conjunto de imágenes de entrenamiento
	\item Disponer de las etiquetas de las imágenes en uno de los formatos soportados
	\item Haber generado los \textit{anchors} para las imágenes de entrenamiento
	\item Haber creado un fichero de configuración
\end{itemize}

Una vez que se dispone de todo lo anterior, lo único que queda por hacer es ejecutar el script \texttt{train.py}:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Cómo lanzar el entrenamiento}, captionpos=b]
> python train.py --config config.json
\end{lstlisting}

Una vez finaliza el entrenamiento, y si se ha configurado el bloque \texttt{valid} en el fichero de configuración, se realizará una evaluación del modelo entrenado.

\subsubsection*{Evaluación}

Para evaluar un modelo YOLO se necesita:

\begin{itemize}
	\item Disponer de un conjunto de imágenes de test
	\item Disponer de las etiquetas de las imágenes en uno de los formatos soportados
	\item Disponer de un modelo ya entrenado
	\item Haber creado un fichero de configuración
\end{itemize}

Una vez se dispone de todo lo anterior, lo único que habrá que hacer es ejecutar el script \texttt{evaluate.py}:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Cómo evaluar un modelo entrenado}, captionpos=b]
> python evaluate.py --config config.json
\end{lstlisting}

La evaluación hace el cálculo de la \textit{AP} tanto de una forma global como para cada una de las clases que se hayan configurado.

\subsubsection*{Predicción}

Para hacer una predicción con el modelo ya entrenado es necesario:

\begin{itemize}
	\item Disponer de una imagen o directorio con imágenes o vídeo
	\item Disponer de un modelo ya entrenado
	\item Haber creado un fichero de configuración
\end{itemize}

Con todo lo anterior, se ejecuta el script \texttt{predict.py}:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, caption={Cómo hacer una predicción con un modelo entrenado}, captionpos=b]
# para una imagen
> python predict.py -c config.json -i /tmp/imagen.jpg -o /tmp/pred/

# para un video
> python predict.py -c config.json -i /tmp/video.mp4 -o /tmp/pred/

# para un conjunto de imagenes
> python predict.py -c config.json -i /tmp/imagenes -o /tmp/pred/
\end{lstlisting}

En el directorio de salida habrá el mismo contenido que en la entrada, modificado con las etiquetas que se han encontrado.

\subsubsection*{Transformación del modelo}

Para explotar un modelo entrenado en un dispositivo móvil es necesario realizar una transformación del mismo. Para realizar esta transformación, \textit{TFLite} proporciona una serie de utilidades. En el siguiente bloque de código se muestra un ejemplo:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, language=Python, caption={Cómo transformar un modelo entrenado con \textit{Keras} a formato \textit{TFLite}}, captionpos=b]
import tensorflow as tf

tflite_converter = tf.lite.TFLiteConverter
  .from_keras_model_file(<KERAS_H5_MODEL_PATH>)

tflite_model = tflite_converter.convert()

with open(<KERAS_TFLITE_DEST_PATH>, 'wb') as tflite_model_file:
  tflite_model_file.write(tflite_model)
\end{lstlisting}

\subsubsection*{Explotación del modelo}

\textit{TFLite} además de proporcionar herramientas para transformar modelos también proporciona librerías para explotarlos de diversas formas. En concreto, proporciona una librería java, que entre otras cosas, permite explotar un modelo en un dispositivo móvil \textit{Android}.

En este primer bloque de código se muestra un ejemplo de cómo se puede cargar un modelo que forma parte de los \textit{assets} de la aplicación móvil:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, language=Java, caption={Cómo cargar un modelo}, captionpos=b]
AssetFileDescriptor fileDescriptor = assets
                                 .openFd(<MODEL_TFLITE_FILENAME>);
FileInputStream inputStream = new FileInputStream(fileDescriptor
                                 .getFileDescriptor());
FileChannel fileChannel = inputStream.getChannel();
long startOffset = fileDescriptor.getStartOffset();
long declaredLength = fileDescriptor.getDeclaredLength();
MappedByteBuffer model = fileChannel.map(
                                 FileChannel.MapMode.READ_ONLY,
                                 startOffset, declaredLength);

Interpreter.Options interpreterOptions = new Interpreter.Options()
                                           .setNumThreads(1);
Interpreter tfLite = new Interpreter(model, interpreterOptions);
\end{lstlisting}

En la variable \texttt{tflite} se tiene disponible el modelo ya cargado, listo para ser explotado. Para ello, hay que proporcionarle los vectores que se espera como entrada e indicarle los vectores donde devolver el resultado. En el siguiente ejemplo se crea un único vector de entrada de dimensiones $1 x 256 x 256 x 3$, que se correspondería con una imagen a color de $256x256$. Y se crean 2 vectores de salida de tamaños $8 x 8 x 18$ y $16 x 16 x 18$, que se corresponden con los vectores de salida de la versión YOLO V3 Tiny con un tamaño de $256$:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, language=Java, caption={Creación de vectores de entrada y de salida del modelo}, captionpos=b]
float[][][][] floatValues = new float[1][256][256][3];

for (int i = 0; i < 256; i++) {
  for (int j = 0; j < 256; j++) {
    for (int k = 0; k < 3; k++) {
      floatValues[0][i][j][k] = 1.0f;
    }
  }
}

Object[] inputArray = {floatValues};

output1 = new float[1][8][8][18];
output2 = new float[1][16][16][18];

Map<Integer, Object> outputMap = new HashMap<>();
outputMap.put(0, output1);
outputMap.put(1, output2);
\end{lstlisting}

El modelo se ejecuta de la siguiente manera:

\begin{lstlisting}[frame=single, basicstyle=\ttfamily\footnotesize, language=Java, caption={Cómo ejecutar el modelo}, captionpos=b]
tfLite.runForMultipleInputsOutputs(inputArray, outputMap);
\end{lstlisting}

Una vez termine de ejecutarse el método \texttt{runForMultipleInputsOutputs}, en la variable \texttt{outputMap} estará disponible la salida del modelo lista para ser interpretada.

\subsection{Evaluación de las técnicas}

Como ya se ha comentado con anterioridad, se han entrenado dos versiones de YOLO: la versión V3 y la versión V3 Tiny.

Para cada una de estas versiones, se han entrenado varios modelos con distintos tamaños de red, por dos motivos principalmente: por una cuestión de rendimiento a la hora de ejecutar el modelo en un dispositivo móvil y por analizar cómo varía la precisión del modelo cambiando el tamaño de la red. La versión YOLO V3 ha sido entrenada con 3 tamaños (640, 416 y 256) y la versión YOLO V3 Tiny con 2 tamaños (416 y 256)

Además, cada de los modelos anteriores ha sido entrenado por triplicado, utilizando distintos conjuntos de entrenamiento en cada ocasión. Se han utilizado 3 conjuntos de entrenamiento diferentes, obteniéndose un total de 15 modelos. El primero de los conjuntos de entrenamiento se corresponde con el conjunto íntegro original (denominado \textit{completo}). Para obtener el segundo de los conjuntos de entrenamiento, se ha aplicado un filtro sobre el conjunto original, seleccionando únicamente los baches con un tamaño superior a 75x30 píxeles (denominado \textit{filtro 75x30}). El tercero de los conjuntos (denominado \textit{filtro 100x40}) es similar como el anterior pero con un filtro diferente, en esta ocasión se han seleccionado los baches con tamaño superior a 100x40 píxeles. Para cada uno de estos conjuntos de entrenamiento filtrados se ha creado también su correspondiente conjunto de evaluación aplicando el mismo filtro.

Con todos los modelos resultantes obtenidos se ha realizado una doble evaluación. Por un lado se han evaluado los modelos con los conjuntos de evaluación correspondientes a cada uno de los conjuntos de entrenamiento (resultados en la tabla \ref{tab:evaluationoriginal}). Por otro lado se han evaluado con un conjunto de imágenes generado (resultados en la tabla \ref{tab:evaluationcustom}). Este conjunto de evaluación (denominado \textit{propio}) se compone de unas 30 imágenes de 4032x3024 píxeles, con unos 60 baches en total, obtenido desde la acera (a diferencia del original que fue obtenido desde el coche) y compuesto por fotos realizadas en España (a diferencia del original que fueron realizadas en Sudáfrica).

\begin{table}[H]
	\centering
	\begin{tabular}{lrlrr}
		\toprule
		Versión YOLO &  Tamaño &    Juego datos &  Épocas &  Mejor AP \\
		\midrule
		V3      &     256 &       completo &      43 &    0.0747 \\
		V3      &     256 &  filtro 100x40 &      93 &    0.3077 \\
		V3      &     256 &   filtro 75x30 &      88 &    0.2513 \\
		V3      &     416 &       completo &      18 &    0.1467 \\
		V3      &     416 &  filtro 100x40 &      93 &    0.4161 \\
		V3      &     416 &   filtro 75x30 &      93 &    0.3611 \\
		V3      &     640 &       completo &      13 &    0.0186 \\
		V3      &     640 &  filtro 100x40 &      63 &    0.5475 \\
		V3      &     640 &   filtro 75x30 &      53 &    0.4106 \\
		V3 Tiny &     256 &       completo &     144 &    0.0046 \\
		V3 Tiny &     256 &  filtro 100x40 &     136 &    0.0510 \\
		V3 Tiny &     256 &   filtro 75x30 &     153 &    0.0392 \\
		V3 Tiny &     416 &       completo &     153 &    0.0145 \\
		V3 Tiny &     416 &  filtro 100x40 &     153 &    0.1307 \\
		V3 Tiny &     416 &   filtro 75x30 &     146 &    0.0869 \\
		\bottomrule
	\end{tabular}
	\caption{Resultados obtenidos con los conjuntos de evaluación originales}
	\label{tab:evaluationoriginal}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{lrlrr}
		\toprule
		Versión YOLO &  Tamaño &    Juego datos &  Épocas &  Mejor AP \\
		\midrule
		V3      &     256 &       propio completo &      43 &    0.0289 \\
		V3      &     256 &  propio filtro 100x40 &      93 &    0.1018 \\
		V3      &     256 &   propio filtro 75x30 &      88 &    0.0179 \\
		V3      &     416 &       propio completo &      18 &    0.0354 \\
		V3      &     416 &  propio filtro 100x40 &      93 &    0.0089 \\
		V3      &     416 &   propio filtro 75x30 &      93 &    0.0294 \\
		V3      &     640 &       propio completo &      13 &    0.0017 \\
		V3      &     640 &  propio filtro 100x40 &      63 &    0.0342 \\
		V3      &     640 &   propio filtro 75x30 &      53 &    0.0961 \\
		V3 Tiny &     256 &       propio completo &     144 &    0.0086 \\
		V3 Tiny &     256 &  propio filtro 100x40 &     136 &    0.0232 \\
		V3 Tiny &     256 &   propio filtro 75x30 &     153 &    0.0371 \\
		V3 Tiny &     416 &       propio completo &     153 &    0.0000 \\
		V3 Tiny &     416 &  propio filtro 100x40 &     153 &    0.0000 \\
		V3 Tiny &     416 &   propio filtro 75x30 &     146 &    0.0006 \\
		\bottomrule
	\end{tabular}
	\caption{Resultados obtenidos con el conjunto de evaluación propio}
	\label{tab:evaluationcustom}
\end{table}